{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ERALP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Training Loss: 0.0014638773295792271, Validation Loss: 0.00031595230321862635\n",
      "Epoch 2/30, Training Loss: 0.00044380844092411717, Validation Loss: 0.00022157599318301197\n",
      "Epoch 3/30, Training Loss: 0.0002930479981787138, Validation Loss: 0.00017440959609996116\n",
      "Epoch 4/30, Training Loss: 0.0002703284497248793, Validation Loss: 0.00024232801854890968\n",
      "Epoch 5/30, Training Loss: 0.00023009841755130956, Validation Loss: 0.00023468151392562792\n",
      "Epoch 6/30, Training Loss: 0.00022567561382860746, Validation Loss: 0.0002059371997857656\n",
      "Epoch 7/30, Training Loss: 0.00017664508253889275, Validation Loss: 0.00013843615381225712\n",
      "Epoch 8/30, Training Loss: 0.0001634550864041393, Validation Loss: 9.334230283760249e-05\n",
      "Epoch 9/30, Training Loss: 0.00016422268972710152, Validation Loss: 0.00037898129433476085\n",
      "Epoch 10/30, Training Loss: 0.0001520766963912901, Validation Loss: 0.00017186758922393622\n",
      "Epoch 11/30, Training Loss: 0.00014838237284237835, Validation Loss: 0.00010155431222829297\n",
      "Epoch 12/30, Training Loss: 0.00016458302904053074, Validation Loss: 9.122960828497372e-05\n",
      "Epoch 13/30, Training Loss: 0.00014032991054909323, Validation Loss: 0.00011090675409848948\n",
      "Epoch 14/30, Training Loss: 0.00013949327846665504, Validation Loss: 8.085034425337015e-05\n",
      "Epoch 15/30, Training Loss: 0.0001136321359253596, Validation Loss: 0.00010005166281864553\n",
      "Epoch 16/30, Training Loss: 0.00011108902452975146, Validation Loss: 7.649428805555584e-05\n",
      "Epoch 17/30, Training Loss: 0.00016975377105019475, Validation Loss: 0.00011887030261185297\n",
      "Epoch 18/30, Training Loss: 0.00015421283732499857, Validation Loss: 0.00028078239184508306\n",
      "Epoch 19/30, Training Loss: 0.0001785749229756801, Validation Loss: 0.00017036547438559966\n",
      "Epoch 20/30, Training Loss: 0.00010937584514078524, Validation Loss: 0.00010913102879684502\n",
      "Epoch 21/30, Training Loss: 9.443908803372192e-05, Validation Loss: 0.0001155343288008286\n",
      "Epoch 22/30, Training Loss: 9.693485814812696e-05, Validation Loss: 0.00024323844716430637\n",
      "Epoch 23/30, Training Loss: 0.00010815276046818632, Validation Loss: 7.618064957642363e-05\n",
      "Epoch 24/30, Training Loss: 0.00010108555991072385, Validation Loss: 7.786747806321574e-05\n",
      "Epoch 25/30, Training Loss: 8.640011211870173e-05, Validation Loss: 6.989337076529961e-05\n",
      "Epoch 26/30, Training Loss: 9.537445355335767e-05, Validation Loss: 0.00014346965592936295\n",
      "Epoch 27/30, Training Loss: 9.454532896981333e-05, Validation Loss: 0.0001584218440796984\n",
      "Epoch 28/30, Training Loss: 0.00013708491656783386, Validation Loss: 8.225621632310097e-05\n",
      "Epoch 29/30, Training Loss: 9.552165128586483e-05, Validation Loss: 0.0003101185250740663\n",
      "Epoch 30/30, Training Loss: 0.00011636193849848365, Validation Loss: 5.923309577994945e-05\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_10400\\2788456719.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m    132\u001B[0m         \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msqueeze\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabels\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    133\u001B[0m         \u001B[0mtest_loss\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0mloss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mitem\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 134\u001B[1;33m         \u001B[0mpredictions\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mextend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msqueeze\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    135\u001B[0m         \u001B[0mtrue_values\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mextend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlabels\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    136\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Reading data\n",
    "\n",
    "akbank_data = pd.read_csv(\n",
    "    \"C:\\\\Users\\\\ERALP\\\\OneDrive\\\\Desktop\\\\Software Engineering\\\\SWE599---Financial-Time-Series-Volatility-and-Return-Forecasting\\\\Code\\\\akbank_data.csv\")\n",
    "\n",
    "akbank_data = akbank_data[['Date', 'Hour', 'Open', 'High', 'Low', 'Close']]\n",
    "\n",
    "#Converting Date and Hour to datetime object and setting it as index. This will help us to plot the data in a time series manner.\n",
    "\n",
    "akbank_data['Datetime'] = pd.to_datetime(akbank_data['Date'] + \" \" + akbank_data['Hour'])\n",
    "\n",
    "#Dropping Date and Hour columns\n",
    "akbank_data.set_index('Datetime', inplace=True)\n",
    "akbank_data.drop(['Date', 'Hour'], axis=1, inplace=True)\n",
    "\n",
    "time_steps = 32\n",
    "\n",
    "\n",
    "def create_sliding_windows(data, time_steps):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data[i:i + time_steps])\n",
    "        y.append(data[i + time_steps, -1])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "#Normalizing the data\n",
    "scaler = MinMaxScaler()\n",
    "akbank_data_scaled = scaler.fit_transform(akbank_data[['Open', 'High', 'Low', 'Close']])\n",
    "\n",
    "#Creating sliding windows\n",
    "X, y = create_sliding_windows(akbank_data_scaled, time_steps)\n",
    "\n",
    "#Splitting the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Converting the data into torch tensors\n",
    "X_train = torch.tensor(X_train).float()\n",
    "X_test = torch.tensor(X_test).float()\n",
    "y_train = torch.tensor(y_train).float()\n",
    "y_test = torch.tensor(y_test).float()\n",
    "\n",
    "\n",
    "#Creating the Convolutional Neural Network model class\n",
    "\n",
    "class StockPricePreditor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StockPricePreditor, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=4, out_channels=32, kernel_size=5)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=5)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64 *5, 128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # Swap the dimensions here\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu3(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "#Creating the model object\n",
    "model = StockPricePreditor()\n",
    "\n",
    "#Defining the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "#Data loader\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "#Training the model\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    #Validating the model\n",
    "\n",
    "    valid_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {running_loss / len(train_loader)}, Validation Loss: {valid_loss / len(test_loader)}\")\n",
    "\n",
    "#Evaluating the model\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "predictions = []\n",
    "true_values = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        test_loss += loss.item()\n",
    "        predictions.extend(outputs.squeeze().numpy())\n",
    "        true_values.extend(labels.numpy())\n",
    "\n",
    "print(f\"Test Loss: {test_loss / len(test_loader)}\")\n",
    "\n",
    "#Converting the predictions and true values to the original scale\n",
    "\n",
    "predictions = np.array(predictions).reshape(-1, 1)\n",
    "true_values = np.array(true_values).reshape(-1, 1)\n",
    "predictions = scaler.inverse_transform(np.hstack([np.zeros((predictions.shape[0], 3)), predictions]))\n",
    "true_values = scaler.inverse_transform(np.hstack([np.zeros((true_values.shape[0], 3)), true_values]))\n",
    "\n",
    "# Calculating the mean squared error and mean absolute error\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "mse = mean_squared_error(true_values[:, -1], predictions[:, -1])\n",
    "mae = mean_absolute_error(true_values[:, -1], predictions[:, -1])\n",
    "\n",
    "print(f\"MSE: {mse}, MAE: {mae}\")\n",
    "\n",
    "#Plotting the results\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(true_values, label='True Values')\n",
    "plt.plot(predictions, label='Predictions')\n",
    "plt.title('True Values vs Predictions')\n",
    "plt.legend()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
